# Typo Fixer CLI

A production-ready CLI tool for fixing typos using fine-tuned Qwen models via [candle-coreml](https://crates.io/crates/candle-coreml). Features **automatic model downloading** from HuggingFace and seamless CoreML inference with Apple Neural Engine acceleration.

## ğŸ¯ Project Goals

This project serves as a **real-world implementation** of candle-coreml, demonstrating:
- âœ¨ **Automatic Model Download**: Zero-setup experience with HuggingFace Hub integration
- ğŸ”§ **Auto-Config Generation**: No manual configuration files needed
- ğŸ§  **Production-Ready ML**: Fine-tuned models with real-world performance
- âš¡ **Apple Neural Engine**: High-performance CoreML inference on Apple Silicon
- ğŸ›ï¸ **Unified Cache Management**: Efficient model storage and reuse

## âœ¨ Features

- ğŸš€ **Zero Setup**: Models download automatically on first use
- ğŸ§  **Fine-tuned Model**: Uses optimized `mazhewitt/qwen-typo-fixer-coreml` 
- ğŸ“ **Few-shot Prompting**: Engineered prompts with typo correction examples
- âš¡ **Apple Neural Engine**: Leverages ANE acceleration via CoreML
- ğŸ¯ **Multiple Formats**: Text, JSON, and verbose output options
- ğŸ“Š **Batch Processing**: Handle multiple lines of input
- ğŸ—‚ï¸ **Smart Caching**: Models cached locally for fast subsequent use

## ğŸš€ Quick Start

### Prerequisites
- macOS (required for CoreML support)
- Rust 1.70+ with Cargo
- Internet connection (for first-time model download)

### Installation

```bash
git clone <repository-url>
cd typo-fixer-cli
cargo build --release
```

### Basic Usage (Zero Setup!)

```bash
# Fix a single sentence - model downloads automatically on first use
./target/release/typo-fixer-cli "this sentance has typoos"

# Use verbose output to see model loading and inference details
./target/release/typo-fixer-cli "recieve the seperate package" --verbose

# JSON output with metadata
./target/release/typo-fixer-cli "definately wrong" --output json

# Process from stdin
echo "the quik brown fox" | ./target/release/typo-fixer-cli --stdin

# Batch mode for multiple lines
./target/release/typo-fixer-cli --batch << EOF
this sentance has typoos
the quik brown fox jumps
i cant beleive this happend
EOF
```

### Using Different Models

```bash
# Use a different HuggingFace CoreML model (downloads automatically)
./target/release/typo-fixer-cli --model "your-org/your-coreml-model" "test text"

# For backward compatibility: use local models with explicit config
./target/release/typo-fixer-cli \
  --local-path "/path/to/model/qwen-typo-fixer-ane" \
  --config "./configs/qwen-typo-fixer-ane.json" \
  "helo wrold"
```

### Advanced Options

```bash
# Adjust generation parameters
./target/release/typo-fixer-cli \
  --temperature 0.0 \
  --max-tokens 10 \
  "quick test"

# See all options
./target/release/typo-fixer-cli --help
```

## ğŸ‰ What's New: Automatic Download System

### âœ¨ Before vs After

**âŒ Old Way (Manual Setup)**:
1. Download model files manually
2. Create JSON configuration files
3. Hardcode absolute paths
4. Break when paths change

**âœ… New Way (Zero Setup)**:
```bash
# Just works! Model downloads and configures automatically
./target/release/typo-fixer-cli "fix my typos"
```

### ğŸ”§ Under the Hood

The new system uses [candle-coreml's UnifiedModelLoader](../src/unified_model_loader.rs):
1. **Auto-Download**: Models downloaded from HuggingFace on first use
2. **Auto-Config**: CoreML .mlpackage files inspected and configured automatically  
3. **Smart Caching**: Models cached in `~/.cache/candle-coreml/` for reuse
4. **Config Generation**: JSON configurations auto-generated from model metadata

### ğŸ“ Cache Structure

Models are organized in a clean cache structure:
```
~/.cache/candle-coreml/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mazhewitt--qwen-typo-fixer-coreml/     # Downloaded .mlpackage files
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ mazhewitt--qwen-typo-fixer-coreml.json # Auto-generated config
â””â”€â”€ temp/                                       # Temporary download files
```

## ğŸ§ª Testing Results

### âœ… Fully Working Features

- [x] **Automatic Model Download**: HuggingFace Hub integration working
- [x] **Auto-Config Generation**: .mlpackage inspection and JSON generation  
- [x] **CLI Argument Parsing**: All argument combinations validated
- [x] **Prompt Engineering**: Few-shot examples for typo correction
- [x] **CoreML Integration**: Full candle-coreml UnifiedModelLoader support
- [x] **Cache Management**: Efficient model storage and cleanup
- [x] **Multi-Component Models**: Handles embeddings, FFN, and LM head components
- [x] **Output Formats**: Text, JSON, and verbose formatting
- [x] **Error Handling**: Graceful failure with helpful messages

### ğŸ”¬ Test Results

All tests pass with the new system:
```bash
$ cargo test
    running 14 tests
    test result: ok. 14 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
```

### ğŸ¯ Performance Characteristics

**Automatic Download System**:
- âœ… **First Run**: ~30-60 seconds (2.5GB model download + compilation)
- âœ… **Subsequent Runs**: ~2-5 seconds (uses cached model)
- âœ… **Model Loading**: Fast with compiled CoreML models
- âœ… **CoreML Inference**: Apple Neural Engine acceleration working
- âœ… **Memory Usage**: Efficient with CoreML optimization

**Architecture Performance**:
- âœ… **Multi-Component Pipeline**: Seamless embeddings â†’ FFN â†’ LM head execution
- âš ï¸ **Generation Speed**: Optimized for quality over speed (single-token architecture)
- âœ… **Cache Efficiency**: Models shared across runs and applications

## ğŸ—ï¸ Architecture

### New Download & Config Flow

```
User Command
     â†“
CLI Parsing
     â†“
UnifiedModelLoader.load_model()
     â†“
â”Œâ”€ Check Cache â”€â”    â”Œâ”€ Download Model â”€â”    â”Œâ”€ Generate Config â”€â”
â”‚ Cached?       â”‚ â†’  â”‚ HuggingFace Hub  â”‚ â†’  â”‚ Inspect .mlpackageâ”‚
â”‚ âœ“ Use cached  â”‚    â”‚ Clean git+LFS    â”‚    â”‚ Auto-detect shapesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€ Load QwenModel â”€â”
â”‚ CoreML compile   â”‚
â”‚ Initialize state â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Text Generation
```

### Code Structure

```
src/
â”œâ”€â”€ main.rs           # CLI entry point with new auto-download flow
â”œâ”€â”€ lib.rs            # Library interface using UnifiedModelLoader  
â”œâ”€â”€ cli.rs            # Updated CLI args with new model defaults
â”œâ”€â”€ prompt.rs         # Few-shot prompt engineering (unchanged)
â””â”€â”€ typo_fixer.rs     # Core logic updated for UnifiedModelLoader

tests/
â”œâ”€â”€ integration_tests.rs  # CLI argument validation
â”œâ”€â”€ model_tests.rs        # HuggingFace integration tests
â”œâ”€â”€ accuracy_tests.rs     # End-to-end functionality with auto-download
â””â”€â”€ real_model_tests.rs   # Tests with new download system
```

## ğŸ¤– candle-coreml Integration Evolution

### âœ… What's New and Working

**New Unified System**:
- âœ¨ **UnifiedModelLoader**: Single API for download + config + loading
- âœ¨ **Auto-Config Generation**: No manual JSON files needed
- âœ¨ **HuggingFace Integration**: Direct model downloads with git+LFS
- âœ¨ **Cache Management**: Organized model and config storage
- âœ… **Apple Neural Engine**: Full CoreML acceleration maintained

**Maintained Excellence**:
- âœ… **Text Generation**: `generate_text()` with temperature and token control
- âœ… **Multi-Component Models**: Seamless embeddings, FFN, and LM head handling
- âœ… **Platform Support**: Graceful handling of macOS vs other platforms
- âœ… **Error Handling**: Comprehensive error messages and recovery

### ğŸ”§ New Integration Patterns

```rust
use candle_coreml::UnifiedModelLoader;

// NEW: Zero-setup model loading
let loader = UnifiedModelLoader::new()?;
let model = loader.load_model("mazhewitt/qwen-typo-fixer-coreml")?;

// Automatic pipeline: download â†’ config â†’ load â†’ ready!
let result = model.generate_text(prompt, max_tokens, temperature)?;
```

**vs Old Manual Way**:
```rust
// OLD: Manual setup required
let model_config = ModelConfig::load_from_file("config.json")?;
let qwen_config = QwenConfig::from_model_config(model_config);
let model = QwenModel::load_from_directory(&model_path, Some(qwen_config))?;
```

### ğŸ›ï¸ Model Architecture Support

**Enhanced Multi-Component Pipeline**:
- ğŸ” **Auto-Discovery**: Components detected from .mlpackage files
- ğŸ“ **Shape Detection**: Tensor shapes auto-inferred from CoreML metadata  
- ğŸ”§ **Config Generation**: Complete ModelConfig created automatically
- âš¡ **Optimized Loading**: Compiled CoreML models cached for performance

**Supported Architectures**:
- âœ… **Single-Token Sequential**: Optimized for memory efficiency
- âœ… **Multi-Part Logits**: 16-part vocabulary output handling
- âœ… **Stateful Processing**: Automatic state management
- âœ… **Batch Processing**: Configurable batch sizes

## ğŸš€ Migration Guide

### ğŸ”„ Updating from Manual Setup

**If you have existing scripts**:

```bash
# Old command (required manual model setup)
./typo-fixer-cli --local-path "/path/to/model" --config "config.json" "text"

# New command (automatic download)  
./typo-fixer-cli "text"  # Uses default mazhewitt/qwen-typo-fixer-coreml

# Or specify different model
./typo-fixer-cli --model "your-org/your-coreml-model" "text"
```

**Local models still supported**:
```bash
# Backward compatibility maintained
./typo-fixer-cli --local-path "/path/to/model" --config "config.json" "text"
```

### ğŸ“¦ For Library Users

```rust
// NEW: Simplified API
use candle_coreml::UnifiedModelLoader;
let loader = UnifiedModelLoader::new()?;
let model = loader.load_model("mazhewitt/qwen-typo-fixer-coreml")?;

// OLD: Manual setup (still works for local models)
use candle_coreml::{QwenModel, QwenConfig, ModelConfig};
let config = ModelConfig::load_from_file("config.json")?;
let model = QwenModel::load_from_directory(&path, Some(QwenConfig::from_model_config(config)))?;
```

## ğŸ”§ Development

### Running Tests

```bash
# Run all unit tests
cargo test

# Test the new download system (requires internet)
cargo test --test accuracy_tests -- --ignored

# Test with verbose output to see download progress
RUST_LOG=info cargo test test_typo_fixer_with_working_model -- --nocapture
```

### Building

```bash
# Debug build
cargo build

# Release build (optimized)
cargo build --release
```

### Cache Management

```bash
# Check what's cached
ls -la ~/.cache/candle-coreml/

# Clear cache if needed (models will re-download)
rm -rf ~/.cache/candle-coreml/
```

## ğŸ‰ Conclusion

This project now demonstrates candle-coreml as a **truly production-ready library** with zero-setup deployment:

### âœ… Revolutionary Improvements

1. **Zero Setup Experience**: From manual configuration to automatic everything
2. **HuggingFace Integration**: Seamless model distribution and updates  
3. **Intelligent Caching**: Efficient storage with automatic cleanup
4. **Auto-Configuration**: No more JSON files to maintain manually
5. **Production Deployment**: Ready for real-world applications

### ğŸ† Key Success Metrics

- **ğŸ¯ Zero Setup**: Models work instantly with no manual configuration
- **ğŸ“ˆ 100% API Compatibility**: All candle-coreml APIs enhanced, not broken
- **âš¡ Performance**: CoreML acceleration with intelligent caching
- **ğŸ› ï¸ Developer Experience**: Simple, intuitive API with comprehensive error handling
- **ğŸš€ Production Ready**: Complete CLI tool with all enterprise features

### ğŸ’¡ Real-World Impact

The enhanced implementation proves that candle-coreml enables developers to:
- **Ship ML Apps Instantly**: No setup barriers for end users
- **Distribute Models Easily**: HuggingFace Hub integration for seamless updates
- **Scale Deployment**: Automatic caching and optimization
- **Focus on Innovation**: Spend time on features, not infrastructure

### ğŸŒŸ From POC to Production

This typo-fixer-cli evolution showcases the full journey:
- **Before**: Manual setup, hardcoded paths, configuration complexity
- **After**: `cargo install typo-fixer-cli && typo-fixer-cli "fix this"`

**This typo-fixer-cli serves as the definitive reference implementation for production candle-coreml applications with automatic model management.**

## ğŸ“š References

- [candle-coreml](https://crates.io/crates/candle-coreml) - The CoreML integration crate
- [UnifiedModelLoader](../src/unified_model_loader.rs) - New automatic download system
- [HuggingFace Hub](https://huggingface.co/mazhewitt/qwen-typo-fixer-coreml) - Model distribution
- [Cache Management](../src/cache_manager.rs) - Intelligent model caching
- [ANEMLL](https://github.com/Anemll/Anemll) - Multi-component ANE optimization